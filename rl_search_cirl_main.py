import gym
import numpy as np
from gym.envs.registration import register

from wrappers import GridPosFeatureWrapper
from cirl_algs import RLSearchCirlAlg

from envs import gridworld, rbfgridworld
import value_iteration as vi


################# FROZEN LAKE #######################
# register(
#     id='FrozenLakeNotSlippery-v0',
#     entry_point='gym.envs.toy_text:FrozenLakeEnv',
#     kwargs={'map_name' : '4x4', 'is_slippery': False},
#     max_episode_steps=100,
#     reward_threshold=0.78, # optimum = .8196
# )
#
# # Generated by 10,000 expert runs on frozen lake (slippery)
# expert_features = np.array(
#     [0.29300975, 0.00480033, 0.00983885, 0.,
#      0.24151374, 0.00251319, 0.01495273, 0.00242812,
#      0.16385533, 0.08998858, 0.0352405,  0.,
#      0.        , 0.0711251,  0.05288627, 0.01784751 ])
#
# env = gym.make('FrozenLakeNotSlippery-v0')


###################  RBF #########################

def expert_features(env, policy, n_trajs=100, len_traj=5):
    """
    env must be a feature env
    """
    feature_trajectories = np.empty((n_trajs, env.d))
    for i in range(n_trajs):
        s = env.reset()

        for j in range(len_traj):
            a = policy[s]
            s, _, done, info = env.step(policy[s])
            if done: break

        # TODO: order of averaging?
        feature_trajectories[i] = env.avg_traj()

    return feature_trajectories.mean(axis=0)


grid = rbfgridworld.RbfGridworldEnv()
# grid = gridworld.GridworldEnv(shape=(5,5))

trans_probs, reward = vi.trans_mat(grid)
U = vi.value_iteration(trans_probs, reward)
policy = vi.best_policy(trans_probs, U)

env = GridPosFeatureWrapper(grid)
expert_features = expert_features(env, policy)

print(expert_features.reshape(9,9))

cirl_test = RLSearchCirlAlg(env, expert_features)

print(cirl_test.cirl_trajectory())
